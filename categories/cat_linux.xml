<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Weird random notes (Posts about linux)</title><link>https://andir.github.io/</link><description></description><atom:link href="https://andir.github.io/categories/cat_linux.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 24 Apr 2017 05:33:05 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>SaltStack, Ansible, Puppet, … how can we share data between servers?</title><link>https://andir.github.io/posts/saltstack-ansible-puppet-how-can-we-share-data/</link><dc:creator>Andreas Rammhold</dc:creator><description>&lt;div&gt;&lt;p&gt;WARNING: This is still a draft somewhat..&lt;/p&gt;
&lt;p&gt;Lately I've been depressed by the lack of automation in my life or really in
the projects I take part in.&lt;/p&gt;
&lt;p&gt;Most of my stuff is not hosted on AWS, Azure, Digitalocean or other big "cloud"
players. That is due to multiple reasons:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We may have hardware, rack, power and network capacity available which is
basically for free.&lt;/li&gt;
&lt;li&gt;We might need physical connectivity to the machine - e.g. connecting to a
local WiFi mesh network, a beamer, some audio equipment and so on…&lt;/li&gt;
&lt;li&gt;We want to remain in control of our data. That means we are not going to
trust some random internet business.&lt;/li&gt;
&lt;li&gt;It is cheaper, even if you've to pay for the servers that are running 24/7.
This might not be true for a website that sells boat trips on the Atlantic
Ocean and gets high traffic spikes every 2nd Sunday from April to December
but for our scenarios it works quite well.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the office I've been using puppet more then six years and I'm quite happy
with it. In the local Freifunk-Community (&lt;a class="reference external" href="https://darmstadt.freifunk.net"&gt;https://darmstadt.freifunk.net&lt;/a&gt;) we
are using SaltStack. For my personal infrastructure (E-Mail, Webhosting, random
docker containers, pastebin, dn42, …) I'm using Ansible (&lt;a class="reference external" href="https://ansible.com"&gt;https://ansible.com&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Neither of those deployments are very trivial, they are probably just the
average project for either of those tools.&lt;/p&gt;
&lt;p&gt;A few weeks back I and a few others started writing SaltStack states for an
IRC-Network that we are running. If you think about automating the deployment
of IRCd's it is rather easier. Create a VM, install your favorite flavor of
linux, install build dependencies for the IRCd of your choice and
generate the configuration. That's basically it.&lt;/p&gt;
&lt;p&gt;We've been using TLS in our network for about 10 years now. (Both user and
server2server communication). Since it wasn't and still is not desirable to buy
wildcard certificates or share certificates between servers we are running our
own CA (including OCSP service etc.). With the switch to automated server
configuration we also want to deploy LetsEncrypt - actually LetsEncrypt
encouraged us to start using some kind of automation since humans aren't
reliable and having to jupe a server every 3 months because the admin still
didn't configure a LetsEncrypt cronjob is not what we are after.&lt;/p&gt;
&lt;p&gt;So I started out writing some states and a Vagrantfile
(&lt;a class="reference external" href="https://gist.github.com/andir/2188da38904557a58cd7df29fd277275"&gt;https://gist.github.com/andir/2188da38904557a58cd7df29fd277275&lt;/a&gt;) for easier
testing of the whole stack. After tackling a few issues with Vagrant and
different &lt;cite&gt;boxes&lt;/cite&gt; (as vagrant calls VM images) everything looked fine.&lt;/p&gt;
&lt;p&gt;We basically have a zoo of VMs on our laptops wich simulate the production
network. At least so we thought.&lt;/p&gt;
&lt;p&gt;For the links between the IRCDs we are using TLS. So far we did exchange
fingerprints, manually add them to the &lt;cite&gt;ircd.conf&lt;/cite&gt;, &lt;cite&gt;/quote rehash&lt;/cite&gt; and off you
go.&lt;/p&gt;
&lt;p&gt;When you automate the deployment you also automate the configuration of links
between your servers. While you can configure &lt;cite&gt;send_password&lt;/cite&gt; and
&lt;cite&gt;receive_password&lt;/cite&gt; since what I do know when that isn't what we want to rely on
when sending chat messages around the globe. So we still need to exchange
certificate fingerprints. One might say that &lt;cite&gt;TLSA&lt;/cite&gt; records are made for that
but I wonder why I can't use the already verified connection between my Salt
Master and the Salt Minion. If you query DNS for TLSA records it requires your
DNS to be up, DNSSEC to work, your local resolver to have a copy of the root
dns zone or an upstream resolver that (hopefully) verifies DNSSEC - unlike
OpenDNS.&lt;/p&gt;
&lt;p&gt;Since we are also running our own DNS Servers adding them as a dependency isn't
really a great idea. We might be recovering from the death of some parts of the
infrastructure. The only reasonable approach to me is to use the authenticated
relationship from minion and master to exchange public keys, fingerprints etc.&lt;/p&gt;
&lt;p&gt;While I'm currently focusing on exchanging certificate fingerprints and/or
public keys this applies to a bunch of scenarios you might come acress when you
decide to drop all shared secrets and generate them on your servers. A few of
those could be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Generating and exchanging TSIG keys for nsupdate between your DNS Servers
and minions.&lt;/li&gt;
&lt;li&gt;Communicating a shared secret for database access, borg backups, …&lt;/li&gt;
&lt;li&gt;Distributing SSHFPs between your servers (local verification, publishing
them to DNS as SSHFP records,…)&lt;/li&gt;
&lt;li&gt;Trust bootstrapping (after minion key verification) for any kind of internal
secret database&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;With SaltStack you've a few options to transfer data from a minion to the master / other minions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;use the Mine to export data&lt;/li&gt;
&lt;li&gt;use grains&lt;/li&gt;
&lt;li&gt;use pillars (with pre-generated certificates, keys, …) -&amp;gt; not the desired solution&lt;/li&gt;
&lt;li&gt;trigger events with custom context&lt;/li&gt;
&lt;li&gt;use a vault&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In puppet you could use exported ressources, in Ansible you could write to some
kind of custom api without much overhead or query/configure the other side
on-demand since a single run isn't restricted to a single server.&lt;/p&gt;
&lt;p&gt;Puppet has solved the issue for many years - people tend to not like puppet for
various reasons. Maybe because it feels like programming?&lt;/p&gt;
&lt;div class="section" id="using-the-salt-mine"&gt;
&lt;h2&gt;Using the Salt Mine&lt;/h2&gt;
&lt;p&gt;The Salt Mine is a handy construct when it comes to exporting data from a node
as long as that data only exists as some kind of list or with a very low
cardinality. Also it only makes sense if the data is mostly static and doesn't
change. Changes to the exported data (e.g. another information, removal of
information, …) require changes to the minion configuration. This isn't really
practical if you are trying to write modular states where not everything is
hard-wired with another state.&lt;/p&gt;
&lt;p&gt;For reference this is how you would export information about a certificate:&lt;/p&gt;
&lt;pre class="code yaml"&gt;&lt;a name="rest_code_f45d57cf70424d58a71058a0b78f77f4-1"&gt;&lt;/a&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mine_functions&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
&lt;a name="rest_code_f45d57cf70424d58a71058a0b78f77f4-2"&gt;&lt;/a&gt;  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;my_awesome_certificate&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
&lt;a name="rest_code_f45d57cf70424d58a71058a0b78f77f4-3"&gt;&lt;/a&gt;    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mine_function&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;tls.cert_info&lt;/span&gt;
&lt;a name="rest_code_f45d57cf70424d58a71058a0b78f77f4-4"&gt;&lt;/a&gt;    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/etc/letsencrypt/live/.../cert.pem&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This has to be deployed on each minion and you probably also have to restart the minion when changeing the file.&lt;/p&gt;
&lt;p&gt;More information on the salt mine can be found at &lt;a class="reference external" href="https://docs.saltstack.com/en/latest/topics/mine/"&gt;https://docs.saltstack.com/en/latest/topics/mine/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="trying-to-export-data-via-grains"&gt;
&lt;h2&gt;Trying to export data via Grains&lt;/h2&gt;
&lt;p&gt;I'm not going to get into the details or provide a example configuration here.
I've written a bunch of grains and they just work. The limitation is that you
can't really attribute them. So you can't tell your custom grains what to
export without changing the code or introducing custom configuration files,
adding stuff to the minion config etc.. While this approach would automatically
propagate new certificate fingerprints, public keys, … you still have to use
the Salt Mine to actually export them.  Which isn't that bad.&lt;/p&gt;
&lt;p&gt;The lack of configuration options for custom grains kills this approach for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pillars"&gt;
&lt;h2&gt;Pillars&lt;/h2&gt;
&lt;p&gt;LOLNOPE. Using pillars would require manual collection of fingerprints or (even
worse) central management of all the certificates.&lt;/p&gt;
&lt;p&gt;This simply doesn't work for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-a-custom-event"&gt;
&lt;h2&gt;Using a custom event&lt;/h2&gt;
&lt;p&gt;This is what seems to be a promising but ephemeral approach to the issue.&lt;/p&gt;
&lt;p&gt;The basic idea is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;Create the public-/private key pair on the minion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;On changes to that fail (creation, key rollover, new certificate, …) execute
a script with the filename as argument&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;The (bash) script then extracts the information we want from the file.
(using &lt;cite&gt;openssl&lt;/cite&gt; or other command line tools) and publishes those
information using something like&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_9c5fc113423d4728a28b21b82d3d4a1e-1"&gt;&lt;/a&gt;&lt;span class="sb"&gt;`&lt;/span&gt;salt-call event.send my/custom/event/certificate-changed &lt;span class="s1"&gt;'{"certfp": "ABCDEF01234567890…"}'&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to use the "published" fingerprint other minions must be up, running
and listening to those events. Otherwise the information is lost and nobody
cares.&lt;/p&gt;
&lt;p&gt;This approach only works if we figure out how to store the fingerprints - after
receiving an event.&lt;/p&gt;
&lt;p&gt;It basically sucks as bad as the others but we might be able to configure the
links after randomly restarting salt-minions and running &lt;cite&gt;salt '*' state.apply&lt;/cite&gt; …&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-a-vault"&gt;
&lt;h2&gt;Using a vault&lt;/h2&gt;
&lt;p&gt;At the time of this writing this seems to be a valid option. I've not tried it
yet. There will be an update on this soon (tm).&lt;/p&gt;
&lt;p&gt;Reading of the vault seems to be rather easy. You can also write to it but only
using infromation that is availbe during state-rendering. So I'm not sure what
the benefit is. One could probably combine this with the event approach to
store the public keys of the minions in the vault by listening to key events on
the master.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This world sucks. We need better tools :/&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>ansible</category><category>automation</category><category>linux</category><category>puppet</category><category>saltstack</category><guid>https://andir.github.io/posts/saltstack-ansible-puppet-how-can-we-share-data/</guid><pubDate>Sat, 22 Apr 2017 16:50:00 GMT</pubDate></item><item><title>Using VRFs with linux</title><link>https://andir.github.io/posts/linux-ip-vrf/</link><dc:creator>Andreas Rammhold</dc:creator><description>&lt;div&gt;&lt;p&gt;Ever since I've heard about VRF support (or VRF-lite like it is called in Documentation/network/vrf.txt) I wanted to start tinkering with it.
Since the topic is currently only covered in the previously mentioned linux kernel documentation I thought it would be a good idea to post some notes.&lt;/p&gt;
&lt;p&gt;It basically boils down to adding an VRF interface and creating two &lt;cite&gt;ip rule&lt;/cite&gt;-entries.&lt;/p&gt;
&lt;p&gt;I'm using a local VM with ArchLinux since the VRF feature seems to require a rather recent kernel. My experience with kernels below version 4.6 weren't that great.&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_c58481ab2f804150b7937152d115dcfd-1"&gt;&lt;/a&gt;$ ip -br link &lt;span class="c1"&gt;# this is where we are start off&lt;/span&gt;
&lt;a name="rest_code_c58481ab2f804150b7937152d115dcfd-2"&gt;&lt;/a&gt;lo               UNKNOWN        &lt;span class="m"&gt;00&lt;/span&gt;:00:00:00:00:00 &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt;
&lt;a name="rest_code_c58481ab2f804150b7937152d115dcfd-3"&gt;&lt;/a&gt;ens3             DOWN           &lt;span class="m"&gt;52&lt;/span&gt;:54:00:12:34:56 &amp;lt;BROADCAST,MULTICAST&amp;gt;
&lt;/pre&gt;&lt;p&gt;Now are adding a new interface named &lt;cite&gt;vrf-customer1&lt;/cite&gt; with the table &lt;cite&gt;customer1&lt;/cite&gt; assigned to it.
The table parameter is used to place routes from your devices within the VRF into the right routing table&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-1"&gt;&lt;/a&gt;$ ip link add vrf-customer1 &lt;span class="nb"&gt;type&lt;/span&gt; vrf table customer1
&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-3"&gt;&lt;/a&gt;$ ip -d link show vrf-customer1 &lt;span class="c1"&gt;# verify that the interface indeed exists and has the correct table assigned to it&lt;/span&gt;
&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-4"&gt;&lt;/a&gt;&lt;span class="m"&gt;4&lt;/span&gt;: vrf-customer1: &amp;lt;NOARP,MASTER&amp;gt; mtu &lt;span class="m"&gt;1500&lt;/span&gt; qdisc noop state DOWN mode DEFAULT group default qlen &lt;span class="m"&gt;1000&lt;/span&gt;
&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-5"&gt;&lt;/a&gt;    link/ether ca:22:59:ba:05:da brd ff:ff:ff:ff:ff:ff promiscuity &lt;span class="m"&gt;0&lt;/span&gt;
&lt;a name="rest_code_03a8f4b9c73e4ad0854057cbc90443d3-6"&gt;&lt;/a&gt;    vrf table &lt;span class="m"&gt;100&lt;/span&gt; addrgenmode eui64
&lt;/pre&gt;&lt;p&gt;Next: redirect the traffic from and to the vrf to the customer1 table and verify the rules are indeed as expected:&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-1"&gt;&lt;/a&gt;$ ip -4 rule add oif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-2"&gt;&lt;/a&gt;$ ip -4 rule add iif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-3"&gt;&lt;/a&gt;$ ip -6 rule add oif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-4"&gt;&lt;/a&gt;$ ip -6 rule add iif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-6"&gt;&lt;/a&gt;$ ip -4 rule
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-7"&gt;&lt;/a&gt;&lt;span class="m"&gt;0&lt;/span&gt;:  from all lookup &lt;span class="nb"&gt;local&lt;/span&gt;
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-8"&gt;&lt;/a&gt;&lt;span class="m"&gt;32764&lt;/span&gt;:      from all iif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-9"&gt;&lt;/a&gt;&lt;span class="m"&gt;32765&lt;/span&gt;:      from all oif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-10"&gt;&lt;/a&gt;&lt;span class="m"&gt;32766&lt;/span&gt;:      from all lookup main
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-11"&gt;&lt;/a&gt;&lt;span class="m"&gt;32767&lt;/span&gt;:      from all lookup default
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-13"&gt;&lt;/a&gt;$ ip -6 rule
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-14"&gt;&lt;/a&gt;&lt;span class="m"&gt;0&lt;/span&gt;:  from all lookup &lt;span class="nb"&gt;local&lt;/span&gt;
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-15"&gt;&lt;/a&gt;&lt;span class="m"&gt;32764&lt;/span&gt;:      from all oif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-16"&gt;&lt;/a&gt;&lt;span class="m"&gt;32765&lt;/span&gt;:      from all iif vrf-customer1 lookup customer1
&lt;a name="rest_code_3f726a3149a04d99b00dc7ac24c85b27-17"&gt;&lt;/a&gt;&lt;span class="m"&gt;32766&lt;/span&gt;:      from all lookup main
&lt;/pre&gt;&lt;p&gt;To make any use of our VRF we will have to add an device to it. In my case I'll add the only available "physical" device &lt;cite&gt;ens3&lt;/cite&gt;.&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_189eca459b534c238de7bb52b4c05aff-1"&gt;&lt;/a&gt;$ ip link &lt;span class="nb"&gt;set&lt;/span&gt; ens3 master vrf-customer1
&lt;a name="rest_code_189eca459b534c238de7bb52b4c05aff-2"&gt;&lt;/a&gt;$ &lt;span class="c1"&gt;# verify the interface is indeed a member of the VRF&lt;/span&gt;
&lt;a name="rest_code_189eca459b534c238de7bb52b4c05aff-3"&gt;&lt;/a&gt;$ ip -br link show master vrf-customer1
&lt;a name="rest_code_189eca459b534c238de7bb52b4c05aff-4"&gt;&lt;/a&gt;ens3             DOWN           &lt;span class="m"&gt;52&lt;/span&gt;:54:00:12:34:56 &amp;lt;BROADCAST,MULTICAST&amp;gt;
&lt;/pre&gt;&lt;p&gt;Now that we've an interface to receive send send packets with it we should consider adding an IP-Address to it. Since IPv6 is enabled per default we don't need to configure a LL-Address for that protocol.&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_54d7c06d5c624933a14f6ca352b1bb88-1"&gt;&lt;/a&gt;$ &lt;span class="c1"&gt;# add an IP to the interface&lt;/span&gt;
&lt;a name="rest_code_54d7c06d5c624933a14f6ca352b1bb88-2"&gt;&lt;/a&gt;$ ip addr add &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1/24 dev ens3
&lt;a name="rest_code_54d7c06d5c624933a14f6ca352b1bb88-3"&gt;&lt;/a&gt;$ ip route show table customer1
&lt;a name="rest_code_54d7c06d5c624933a14f6ca352b1bb88-4"&gt;&lt;/a&gt;&lt;span class="nb"&gt;local&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1 dev ens3  proto kernel  scope host  src &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;/pre&gt;&lt;p&gt;Seeing a route like that might confuse the average linux user. Those routes usually exist within the local table which you can check via &lt;cite&gt;ip route show table local&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;The route to the /24 we've added is still missing from the interface. Why is that?
You'll have to change the state of the interface to "UP":&lt;/p&gt;
&lt;pre class="code shell"&gt;&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-1"&gt;&lt;/a&gt;$ ip link &lt;span class="nb"&gt;set&lt;/span&gt; ens3 up
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-2"&gt;&lt;/a&gt;$ ip route show table customer1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-3"&gt;&lt;/a&gt;broadcast &lt;span class="m"&gt;10&lt;/span&gt;.0.0.0 dev ens3  proto kernel  scope link  src &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-4"&gt;&lt;/a&gt;&lt;span class="m"&gt;10&lt;/span&gt;.0.0.0/24 dev ens3  proto kernel  scope link  src &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-5"&gt;&lt;/a&gt;&lt;span class="nb"&gt;local&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1 dev ens3  proto kernel  scope host  src &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-6"&gt;&lt;/a&gt;broadcast &lt;span class="m"&gt;10&lt;/span&gt;.0.0.255 dev ens3  proto kernel  scope link  src &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-9"&gt;&lt;/a&gt;$ ip -6 route show table customer1
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-10"&gt;&lt;/a&gt;&lt;span class="nb"&gt;local&lt;/span&gt; fe80::5054:ff:fe12:3456 dev lo  proto none  metric &lt;span class="m"&gt;0&lt;/span&gt;  pref medium
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-11"&gt;&lt;/a&gt;fe80::/64 dev ens3  proto kernel  metric &lt;span class="m"&gt;256&lt;/span&gt;  pref medium
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-12"&gt;&lt;/a&gt;ff00::/8 dev vrf-customer1  metric &lt;span class="m"&gt;256&lt;/span&gt;  pref medium
&lt;a name="rest_code_763608e008b3408f97c911e333e3ad1a-13"&gt;&lt;/a&gt;ff00::/8 dev ens3  metric &lt;span class="m"&gt;256&lt;/span&gt;  pref medium
&lt;/pre&gt;&lt;p&gt;suddenly routes \o/&lt;/p&gt;&lt;/div&gt;</description><category>iproute2</category><category>linux</category><category>routing</category><category>vrf</category><guid>https://andir.github.io/posts/linux-ip-vrf/</guid><pubDate>Tue, 14 Jun 2016 17:00:00 GMT</pubDate></item><item><title>Using multiple client classes with ISC DHCPd</title><link>https://andir.github.io/posts/isc-dhcpd-multiple-classes/</link><dc:creator>Andreas Rammhold</dc:creator><description>&lt;div&gt;&lt;p&gt;Since the internet is lacking examples of how to use multiple classes with a single pool here is one:&lt;/p&gt;
&lt;pre class="code nginx"&gt;&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="s"&gt;"mac-filtered-clients"&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-3"&gt;&lt;/a&gt;    &lt;span class="kn"&gt;match&lt;/span&gt; &lt;span class="s"&gt;binary-to-ascii&lt;/span&gt; &lt;span class="s"&gt;(16,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="s"&gt;,&lt;/span&gt; &lt;span class="s"&gt;":",&lt;/span&gt; &lt;span class="s"&gt;substring(hardware,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="s"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="s"&gt;))&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;subclass&lt;/span&gt; &lt;span class="s"&gt;"mac-filtered-clients"&lt;/span&gt; &lt;span class="s"&gt;"50:7b:00:00:00:00"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;# some cool host!&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="s"&gt;"J-client"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-9"&gt;&lt;/a&gt;    &lt;span class="kn"&gt;spawn&lt;/span&gt; &lt;span class="s"&gt;with&lt;/span&gt; &lt;span class="s"&gt;option&lt;/span&gt; &lt;span class="s"&gt;agent.circuit-id&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-10"&gt;&lt;/a&gt;    &lt;span class="kn"&gt;match&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;(substring(option&lt;/span&gt; &lt;span class="s"&gt;agent.circuit-id,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="s"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="s"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"foo-bar")&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-11"&gt;&lt;/a&gt;    &lt;span class="kn"&gt;lease&lt;/span&gt; &lt;span class="s"&gt;limit&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-12"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-14"&gt;&lt;/a&gt;&lt;span class="k"&gt;subnet&lt;/span&gt; &lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="s"&gt;.168.0.0&lt;/span&gt; &lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="s"&gt;.255.0.0&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-15"&gt;&lt;/a&gt;     &lt;span class="kn"&gt;pool&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-16"&gt;&lt;/a&gt;          &lt;span class="kn"&gt;range&lt;/span&gt; &lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="s"&gt;.168.0.10&lt;/span&gt; &lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="s"&gt;.168.0.150&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-17"&gt;&lt;/a&gt;          &lt;span class="kn"&gt;allow&lt;/span&gt; &lt;span class="s"&gt;members&lt;/span&gt; &lt;span class="s"&gt;of&lt;/span&gt; &lt;span class="s"&gt;"J-client"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-18"&gt;&lt;/a&gt;          &lt;span class="kn"&gt;allow&lt;/span&gt; &lt;span class="s"&gt;members&lt;/span&gt; &lt;span class="s"&gt;of&lt;/span&gt; &lt;span class="s"&gt;"mac-filtered-clients"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-19"&gt;&lt;/a&gt;     &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_be5adfe0dd1f4a1a99142f8407b81199-20"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This isn't very special compared to a setup with just a single class but it can be confusing since debugging classes is a pita.. One pitfall I did run into was using the byte representation of the mac-addresses (without the quotes) and using &lt;code&gt;match hardware;&lt;/code&gt;. The example above works for me (tm).&lt;/p&gt;&lt;/div&gt;</description><category>dhcpd</category><category>isc</category><guid>https://andir.github.io/posts/isc-dhcpd-multiple-classes/</guid><pubDate>Mon, 23 May 2016 11:00:00 GMT</pubDate></item><item><title>Postgresql-tmpfs with systemd.socket-activation for local (ephemeral) data during development</title><link>https://andir.github.io/posts/postgresql-tmpfs-with-sytemdsocket-activation-for-local-ephemeral-data-during-development/</link><dc:creator>Andreas Rammhold</dc:creator><description>&lt;div&gt;&lt;p&gt;During development of database related stuff you commonly run into the "issue" (or non-issue depending on your taste) of running a local database server - or multiple of those.&lt;/p&gt;
&lt;p&gt;In my case I have to run a local postgresql server on my notebook. I asked myself: I'm not always developing on that piece of software, and I do not always require or want a local postgresql server. What can I do about that?!?&lt;/p&gt;
&lt;p&gt;On top of that using my precious SSD to store data I am going to delete anyway souds like a waste (or money). In my development environment I can and want to safely wipe the data often. Also most of the database load comes from running test cases anyway. That stuff doesn't need to end up on my (slow, compared to RAM) disk. Using a tmpfs for that kind of stuff sounds much saner to me.&lt;/p&gt;
&lt;p&gt;The part of running a repetitive clean database setup sounded like the use case for a container based thing. These days docker is pretty "hot" and it solves the issue of distributing re-useable images. There is an official postgresql image on docker hub for various versions of postgresql. I've simply build a new image based on that. It is available on docker hub (&lt;a class="reference external" href="https://hub.docker.com/r/andir/postgresql-tmpfs/"&gt;https://hub.docker.com/r/andir/postgresql-tmpfs/&lt;/a&gt;) or if you prefer to build it on your own you can download the Dockerfile on GitHub (&lt;a class="reference external" href="https://github.com/andir/postgresql-tmpfs"&gt;https://github.com/andir/postgresql-tmpfs&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Now that we are past the introductional blabla here are the systemd unit files I'm using to achieve this:&lt;/p&gt;
&lt;script src="https://gist.github.com/d8307bcead6d83945db462698163ff40.js"&gt;&lt;/script&gt;&lt;noscript&gt;&lt;pre class="literal-block"&gt;
[Service]
ExecStartPre=-/usr/bin/docker rm psql-tmpfs
ExecStart=/usr/bin/docker run --rm --shm-size=2g -name psql-tmpfs -p 127.0.0.1:5434:5432 -t andir/postgresql-tmpfs
ExecStartPost=/bin/sleep 15
ExecStop=/usr/bin/docker stop psql-tmpfs

&lt;/pre&gt;
&lt;/noscript&gt;&lt;p&gt;You can either put those unit files in &lt;cite&gt;/etc/systemd/system&lt;/cite&gt; or install them as systemd-user units in &lt;cite&gt;~/.config/systemd/user&lt;/cite&gt;.&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_ab0f83cec1b54498b9a461aa87b311ec-1"&gt;&lt;/a&gt;systemctl daemon-reload
&lt;a name="rest_code_ab0f83cec1b54498b9a461aa87b311ec-2"&gt;&lt;/a&gt;systemctl &lt;span class="nb"&gt;enable&lt;/span&gt; postgresql-docker.socket
&lt;/pre&gt;&lt;p&gt;If you try to connect to the postgresql server (&lt;code&gt;nc 127.0.0.1 5432&lt;/code&gt;) you can observe the container while it is starting (&lt;code&gt;journalctl -f&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The default username, password and database name is &lt;cite&gt;postgres&lt;/cite&gt;. You can change that by modifying the startup arguments of the docker container. Those are documented at &lt;a class="reference external" href="https://hub.docker.com/_/postgres/"&gt;https://hub.docker.com/_/postgres/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy data trashing \o/&lt;/p&gt;
&lt;p&gt;P.S.: If you've an idea on how to stop the service after x minutes of inactivity please let me know. Stopping the service manually isn't really what I'm after.&lt;/p&gt;&lt;/div&gt;</description><category>docker</category><category>postgresql</category><category>systemd</category><category>tmpfs</category><guid>https://andir.github.io/posts/postgresql-tmpfs-with-sytemdsocket-activation-for-local-ephemeral-data-during-development/</guid><pubDate>Fri, 22 Apr 2016 09:50:09 GMT</pubDate></item></channel></rss>